[reference]
	http://blog.zhiqiangma.info/programming/hadoop-tutorial/
	http://ole-martin.net/hbase-tutorial-for-beginners/
	http://hadoop.apache.org/hbase/docs/current/api/overview-summary.html#overview_description
[setting]
192.168.2.190 [Name: node190, Nature: NameNode, TaskTracker, Hadoop Master and Slave, Hbase Master]
192.168.2.198 [Name: node198, Nature: DataNode, JobTracker, Hadoop Master and Slave, Hbase Slave]
192.168.2.181 [Name: node181, Nature: DataNode, TaskTracker, Hadoop Slave, Hbase Slave]

[setup]
	1. install java jdk 1.6 (all nodes)
		mkdir /usr/java
		cd /usr/java/
		curl -L -O http://mirrors.dotsrc.org/jpackage/1.7/generic/SRPMS.non-free/java-1.6.0-sun-1.6.0.11-1jpp.nosrc.rpm
		chmod +x jdk-6u17-linux-i586-rpm.bin
		./jdk-6u17-linux-i586-rpm.bin
		read agreement and agree
		vi /etc/profile
			export JAVA_HOME=/usr/java/jdk1.6.0_10
			export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
			export PATH=$PATH:$JAVA_HOME/bin
			setenforce 0
			service iptables stop
			
	2. edit /etc/hosts (all nodes)
		192.168.2.190 node190
		192.168.2.198 node198
		192.168.2.181 node181 # will

	3. switch off security for easy testing
		setenforce 0
		service iptables stop
		
	4. add authenticate key (node190)
		ssh-keygen -t rsa
		chmod 755 .ssh
		cat .ssh/id_rsa.pub >> .ssh/authorized_keys
		chmod 755 .ssh/authorized_keys
		scp -r .ssh node164:/root/
		scp -r .ssh node181:/root/
		
	5. install and configure hadoop (node190)
		tar -xzf hadoop-0.20.1.tar.gz
		mv hadoop-0.20.1 hadoop
		mv hadoop-0.20.1.tar.gz ../
		vi hadoop/conf/hadoop-env.sh
			export JAVA_HOME=/usr/java/jdk1.6.0_17
			export HADOOP_HOME=/root/hadoop
		vi hadoop/conf/core-site.xml
			<property>
				<name>fs.default.name</name>
				<value>hdfs://node190:9000/</value>
			</property>
		vi hadoop/conf/mapred-site.xml
			<property>
				<name>mapred.job.tracker</name>
				<value>node198:9001</value>
			</property>
		vi hadoop/conf/masters
			node190
			node198
		vi hadoop/conf/slaves
			node190
			node198
			node181
		copy to node198 and node181
			scp -r /root/hadoop node198:/root/
			scp -r /root/hadoop node181:/root/
		format a new dfs (node190)
			./hadoop namenode -format
		./start-dfs.sh (node190)
		./hadoop dfsadmin -report
		./start-mapred.sh (node198)
		./hadoop job -list
		Copy the input files into the distributed filesystem:
			bin/hadoop fs -put conf input

		Run some of the examples provided:
			$ bin/hadoop jar hadoop-*-examples.jar grep input output 'dfs[a-z.]+'
		
		
	6. install and configure hbase (node190)
		tar -zxf hbase-0.20.2.tar.gz
		mv hbase
		mv hbase-0.20.2 hbase
		vi hbase/conf/hbase-env.sh
		vi hbase/conf/hbase-site.xml
			<property>
				<name>hbase.rootdir</name>
				<value>hdfs://node190:9000/hbase</value>
				<description>The directory shared by region servers.
				</description>
			</property>
			?<property>
				?<name>hbase.cluster.distributed</name>
				?<value>true</value>
				?<description>The mode the cluster will be in. Possible values are
					false: standalone and pseudo-distributed setups with managed Zookeeper
					true: fully-distributed with unmanaged Zookeeper Quorum (see hbase-env.sh)
				?</description>
			?</property>
			<property>
				<name>hbase.master</name>
				<value>node190:60000</value>
				<description>The host and port that the HBase master runs at.
				</description>
			</property>
		vi hbase/conf/regionservers
			node190
			node198
			node181
		scp -r /root/hbase node198:/root/
		scp -r /root/hbase node181:/root/
		./start-hbase.sh
		./hbase shell
		